# 专题18：一致性协议（史上最全、定期更新）

## 一致性协议
常见的一致性协议有二阶段提交（2PC）、三阶段提交（3PC）、Paxos、Raft等算法，本文将介绍其中的一部分。  


### 2PC
2PC即Two-Phase Commit（二阶段提交），广泛应用在数据库领域，目的是让基于分布式架构的所有节点在进行事务处理时保持原子性和一致性。绝大部分关系型数据库均基于2PC完成分布式事务处理。  

顾名思义，2PC分为两个阶段处理：  

<img width="801" height="402" alt="image" src="https://github.com/user-attachments/assets/63ac5aba-fc77-4c14-91ef-fd3d3f32462e" />


#### 阶段一：提交事务请求

1. **事务询问**：协调者向所有参与者发送事务内容，询问是否可以执行提交操作，并开始等待各参与者的响应；  

2. **执行事务**：各参与者节点执行事务操作，并将Undo（回滚日志）和Redo（重做日志）操作计入本机事务日志；  

3. **反馈响应**：各参与者向协调者反馈事务问询的结果——成功执行则返回Yes，否则返回No。  


<img width="805" height="430" alt="image" src="https://github.com/user-attachments/assets/91ef0ec7-5236-4b7b-82da-0348b33939ee" />


#### 阶段二：执行事务提交
协调者根据阶段一的反馈结果，决定是否最终执行事务提交，分为两种情形：  

##### 情形1：执行事务提交（所有参与者均返回Yes）


1. 发送提交请求：协调者向所有参与者发送Commit请求；  

2. 事务提交：参与者收到Commit请求后，正式执行事务提交操作，完成后释放事务执行期间占用的资源；  

3. 反馈提交结果：参与者完成提交后，向协调者发送Ack消息确认；  

4. 完成事务：协调者收到所有参与者的Ack消息后，事务完成。  

##### 情形2：中断事务（存在参与者返回No或等待超时）

只要协调者无法收到所有参与者的Yes响应（如某参与者返回No、或等待超时），就会中断事务：  

1. 发送回滚请求：协调者向所有参与者发送Rollback请求；  

2. 事务回滚：参与者收到请求后，利用本机Undo日志执行回滚操作，完成后释放事务占用的系统资源；  

3. 反馈回滚结果：参与者完成回滚后，向协调者发送Ack消息；  

4. 中断事务：协调者收到所有参与者的回滚Ack消息后，事务中断。  



#### 2PC的优缺点
- **优点**：实现原理简单，易于理解和部署。  
- **缺点**：  

  1. **阻塞问题**：事务执行过程中，所有参与事务的节点均处于阻塞状态，需等待其他参与者响应，无法进行其他操作；  

  2. **单点风险**：协调者是单点，若协调者故障，其他参与者无法释放事务资源，也无法完成事务操作；  

  3. **数据不一致**：若协调者发送Commit请求后出现网络异常或自身崩溃，可能导致部分参与者收到并执行Commit，部分未收到，造成系统数据不一致；  

  4. **容错性差**：无完善的容错机制，参与者故障时，协调者只能依赖超时设置判断后续操作，灵活性低。  


### 3PC
针对2PC的缺点，研究者提出了3PC（Three-Phase Commit，三阶段提交）。作为2PC的改进版，3PC将原有的两阶段重新划分为CanCommit、PreCommit和doCommit三个阶段。  

<img width="642" height="534" alt="image" src="https://github.com/user-attachments/assets/0f00313e-1918-4b71-9c51-8e7ce4397a4c" />


#### 阶段一：CanCommit

1. **事务询问**：协调者向所有参与者发送包含事务内容的CanCommit请求，询问是否可以执行事务提交，并等待应答；  

2. **反馈响应**：正常情况下，参与者若认为可顺利执行事务则返回Yes，否则返回No。  



#### 阶段二：PreCommit
协调者根据阶段一的反馈结果，决定是否执行事务预提交，分为两种情形：  

##### 情形1：执行事务预提交（所有参与者均返回Yes）


1. 发送预提交请求：协调者向所有节点发出PreCommit请求，并进入prepared阶段；  

2. 事务预提交：参与者收到PreCommit请求后，执行事务操作，将Undo和Redo日志写入本机事务日志；  

3. 反馈预提交结果：参与者成功执行事务操作后，向协调者发送Ack响应，同时等待最终的Commit或Abort指令。  


##### 情形2：中断事务（存在参与者返回No或等待超时）
若任意参与者返回No，或协调者等待超时未收到所有响应，则中断事务：  


1. 发送中断请求：协调者向所有参与者发送Abort请求；  


2. 中断事务：参与者无论是否收到Abort请求，若等待协调者指令超时，均会中断事务。  


#### 阶段三：doCommit
此阶段将真正执行事务提交，同样分为两种情形：  

##### 情形1：执行提交（协调者收到所有参与者的Ack响应）

1. 发送提交请求：协调者从预提交状态转换为提交状态，向所有参与者发送doCommit请求；  

2. 事务提交：参与者收到doCommit请求后，正式执行事务提交，完成后释放占用资源；  

3. 反馈提交结果：参与者完成提交后，向协调者发送Ack消息；  

4. 完成事务：协调者收到所有Ack消息后，事务完成。  


##### 情形2：中断事务（协调者收到No响应或等待超时）
若协调者收到任一个参与者的No响应，或超时未收到反馈，则中断事务：  

1. 发送中断请求：协调者向所有参与者发送Abort请求；  

2. 事务回滚：参与者收到Abort请求后，利用阶段二的Undo日志执行回滚，完成后释放资源；  

3. 反馈回滚结果：参与者完成回滚后，向协调者发送Ack消息；  

4. 中断事务：协调者收到所有Ack消息后，事务中断。  



#### 3PC的优缺点
- **优点**：有效降低了2PC中参与者的阻塞范围，且在协调者单点故障后，集群仍可通过超时机制继续达成一致。  
- **缺点**：若参与者收到PreCommit消息后，网络出现分区导致协调者与参与者无法通信，参与者等待超时后会默认执行事务提交，可能造成系统数据不一致。  


### Paxos协议
#### Paxos协议解决的问题
2PC和3PC均需引入协调者角色，若协调者故障，整个事务无法提交，参与者资源会被锁定，对系统影响极大；且出现网络分区时，易导致数据不一致。Paxos协议无需协调者，每个参与者既是参与者也是决策者，能在网络分区场景下最大程度保证一致性。  

Paxos算法由Lamport于1990年提出，是一种基于消息传递的一致性算法。最初因难以理解未受重视，1998年Lamport重新发表后仍未普及；直至2006年Google发表关于Chubby锁服务的论文，其中提到ChubbyCell使用Paxos保证一致性，该算法才得到广泛关注。  

Paxos协议的核心是：在分布式系统中，多个节点就某个值（提案）达成一致（决议），即使少数节点离线，剩余多数节点仍能达成一致。  


#### Paxos协议的角色（可由同一台机器承担）
由于Paxos与下文ZAB协议（ZooKeeper采用）过于相似，详细讲解可参照ZAB协议部分。分布式系统中节点通信有“共享内存”和“消息传递”两种模型，Paxos基于消息传递模型，暂不考虑消息篡改（拜占庭错误）——因网络环境多为自建内网，消息安全性较高。  

Paxos算法需解决的核心问题：在可能出现“进程慢/被杀死/重启”“消息延迟/丢失/重复”的分布式系统中，保证决议一致性，不受上述异常影响。  

Paxos算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发
生以上任何异常，都不会破坏决议的一致性。

Paxos协议的三类核心角色：  
- **提议者（Proposer）**：提议一个值；  
- **接受者（Acceptor）**：对每个提议进行投票；  
- **告知者（Learner）**：被告知投票结果，不参与投票过程。  

<img width="632" height="452" alt="image" src="https://github.com/user-attachments/assets/ffb06e0c-a8f3-4129-b64e-db365f3d4175" />


#### Paxos协议的执行过程
规定一个提议包含两个字段：`[n, v]`，其中`n`为具有全局唯一性的序号，`v`为提议值。以下以“2个Proposer（提案者）+3个Acceptor（接受者）”的系统为例，说明执行流程：  

<img width="719" height="611" alt="image" src="https://github.com/user-attachments/assets/4daaaa72-8dfc-4322-858f-fd15a22e98ff" />


##### 步骤1：Acceptor处理提议请求
Acceptor收到提议请求后，根据是否接收过历史提议，按规则处理：  
- **未接收过任何提议**：发送“无历史提案”（[no previous]）的响应，将当前提议`[n1, v1]`记录为本地已接收提议，并承诺“不再接受序号小于n1的提议”。  
  例：Acceptor X收到`[n=2, v=8]`（Proposer A发送），因无历史提案，发送[no previous]响应，记录`[2,8]`并承诺不接受n<2的提议；Acceptor Y同理。  

<img width="775" height="503" alt="image" src="https://github.com/user-attachments/assets/a92add0f-fcde-46d7-abfe-f99ae733c9e2" />


- **已接收过历史提议`[n1, v1]`**：收到新提议`[n2, v2]`后：  
  1. 若`n1 > n2`：丢弃新提议请求；  

  2. 若`n1 ≤ n2`：发送包含历史提议`[n1, v1]`的响应，更新本地已接收提议为`[n2, v2]`，并承诺“不再接受序号小于n2的提议”。  
  
  例：  
  - Acceptor Z已接收`[n=4, v=5]`（Proposer B发送），后续收到Proposer A的`[n=2, v=8]`，因4>2，丢弃该请求；  
  - Acceptor X已接收`[n=2, v=8]`，后续收到Proposer B的`[n=4, v=5]`，因2≤4，发送`[2,8]`响应，更新本地记录为`[4,5]`并承诺不接受n<4的提议；Acceptor Y同理。  

<img width="785" height="512" alt="image" src="https://github.com/user-attachments/assets/497f6b4d-a5f7-4bd0-bf48-47148a3ec8a4" />

<img width="804" height="502" alt="image" src="https://github.com/user-attachments/assets/e38512ef-e0bd-4db1-bc37-9ce25d426fc7" />


##### 步骤2：Proposer发送接受请求
当Proposer收到**超过一半Acceptor的提议响应**时，可发送接受请求，且接受请求的`v`值需取其收到的所有响应中“最大序号对应的v值”。  
- 例：Proposer A收到2个响应后，发送`[n=2, v=8]`接受请求，但此时所有Acceptor已承诺不接受n<4的提议，故请求被丢弃；Proposer B收到2个响应（含`[2,8]`），取最大v值8，发送`[n=4, v=8]`接受请求。  

<img width="649" height="623" alt="image" src="https://github.com/user-attachments/assets/63204930-c635-47a8-8b69-bdc896ffb29c" />


##### 步骤3：Acceptor通知Learner与最终决议
- Acceptor收到接受请求后，若请求序号**≥自身承诺的最小序号**，则向所有Learner发送通知；  
- Learner发现“大多数Acceptor接收了某个提议”时，该提议的`v`值即被Paxos选择为最终决议。  

<img width="751" height="429" alt="image" src="https://github.com/user-attachments/assets/c26df883-b295-4ba1-9460-6ccfe5eee106" />


### Raft协议
Paxos协议论证了一致性协议的可行性，但理论晦涩、缺少实现细节，工程实现难度高（广为人知的实现仅有ZooKeeper的ZAB协议）。为解决这一问题，斯坦福大学RamCloud项目提出了Raft协议——一种易理解、易实现的分布式一致性复制协议，目前Java、C++、Go等语言均有对应的实现。  

Raft协议的核心改进是“引入主节点（Leader），通过竞选产生”，简化了一致性决策流程。  


#### Raft的核心概念
##### 1. 节点类型
- **Leader（主节点）**：接受客户端更新请求，写入本地后同步到其他副本；  
- **Follower（从节点）**：从Leader接收更新请求，写入本地日志文件，为客户端提供读请求；  
- **Candidate（候选节点）**：若Follower在超时时间内未收到Leader的心跳包，判断Leader可能故障，进入竞选阶段，节点状态从Follower转为Candidate，直至选主结束。  

##### 2. 关键名词
- **termId（任期号）**：时间被划分为多个任期，每次选举后产生新的termId，一个任期内仅存在一个Leader；termId相当于Paxos的proposalId（提案序号）。  
- **RequestVote（请求投票）**：Candidate在选举过程中发起，收到多数派（quorum）响应后成为Leader。  
- **AppendEntries（附加日志）**：Leader发送日志和心跳的机制。  
- **election timeout（选举超时）**：若Follower在一段时间内未收到任何消息（追加日志或心跳），则触发选举超时。  


#### Raft的竞选阶段流程
##### 场景1：初始选主（无Leader）
1. 分布式系统初始阶段仅有Follower（如Node A、Node B、Node C），均处于term=0状态；  

<img width="426" height="369" alt="image" src="https://github.com/user-attachments/assets/a893e4ee-f199-4492-8c09-05d81ef0b00b" />


2. Follower A等待随机竞选超时时间后，未收到Leader心跳，进入竞选阶段，状态转为Candidate，termId更新为1，向其他所有节点发送投票请求（投自己）；  

<img width="464" height="369" alt="image" src="https://github.com/user-attachments/assets/f3da3435-4b21-4680-8c69-0b04effaf909" />


3. 其他节点（如Node B、Node C）收到投票请求后，若未投过票，向Node A回复同意，Node A的投票数达到多数（≥2），状态转为Leader；  

<img width="493" height="399" alt="image" src="https://github.com/user-attachments/assets/99eb1014-7255-4298-9f53-827f875a0a87" />


4. Leader（Node A）周期性向所有Follower发送心跳包，Follower收到心跳后重新计时，避免触发选举超时。  

<img width="433" height="393" alt="image" src="https://github.com/user-attachments/assets/0688a038-12d4-474a-bbf6-a4079fec4287" />


##### 场景2：多个Candidate竞选（票数相同）
1. 若多个Follower同时转为Candidate（如Node B、Node D），且均获得相同票数（如各2票），则选举失败，需重新投票；  

<img width="404" height="377" alt="image" src="https://github.com/user-attachments/assets/4a366430-5a27-4e63-8dba-df3548f351d4" />


2. 重新投票时，因每个节点的随机竞选超时时间不同，再次出现“多个Candidate票数相同”的概率极低，最终会产生唯一Leader。  

<img width="468" height="391" alt="image" src="https://github.com/user-attachments/assets/f03392a6-25d3-4354-aa1a-b2808bf267c9" />


#### Raft的日志复制流程
1. 客户端的修改请求传入Leader，Leader将修改写入本地日志（未提交）；  

<img width="732" height="405" alt="image" src="https://github.com/user-attachments/assets/ca388882-83cb-4ad3-a102-96830601aee4" />


2. Leader将该修改复制到所有Follower；  

<img width="629" height="406" alt="image" src="https://github.com/user-attachments/assets/edbf1917-8405-46f1-bbe2-6e7e714df176" />


3. Leader等待“大多数Follower完成修改写入”后，将本地日志中的修改标记为“已提交”；

<img width="737" height="395" alt="image" src="https://github.com/user-attachments/assets/1075f49c-285c-4934-8fd2-9579ee60737d" />

   
4. Leader通知所有Follower将该修改标记为“已提交”，此时所有节点的值达成一致。  

<img width="694" height="414" alt="image" src="https://github.com/user-attachments/assets/d87005c6-294b-4ab1-8bc3-219ce24a27e9" />

### ZAB协议
#### ZAB协议概述
Google Chubby锁服务的开发者Burrows曾说：“所有一致性协议本质上要么是Paxos，要么是其变体”。Paxos虽解决了分布式节点一致性问题，但存在“多Proposer竞争导致选举速度慢”的问题——当3个及以上Proposer同时发送prepare请求时，难有Proposer收到半数以上响应，导致协议第一阶段反复执行。  

ZooKeeper在Paxos基础上提出ZAB协议（ZooKeeper Atomic Broadcast Protocol），核心改进是“仅允许一台机器作为Proposer（称为Leader），其他节点作为Acceptor”，同时引入Leader选举机制保证Leader健壮性。  

ZAB协议还解决了以下关键问题：  
1. 半数以下节点宕机时，集群仍能提供服务；  
2. 客户端所有写请求交由Leader处理，写入成功后同步给所有Follower和Observer；  
3. Leader宕机或集群重启时，确保Leader已提交的事务最终被所有服务器提交，且集群能快速恢复到故障前状态。  


#### ZAB协议的基本概念
##### 1. 核心名词
- **数据节点（dataNode）**：ZooKeeper数据模型的最小单元，数据模型为树结构，由斜杠（/）分割的路径名唯一标识；数据节点可存储数据、属性信息，还可挂载子节点，构成层次化命名空间。  
- **事务及zxid**：事务是改变ZooKeeper服务器状态的操作（如创建/删除数据节点、更新节点内容、客户端会话创建/失效等）。每个事务请求对应一个全局唯一的事务ID（zxid），为64位数字：  
  - 高32位：事务发生的集群选举周期（每发生一次Leader选举，值加1）；  
  - 低32位：事务在当前选举周期内的递增次序（Leader每处理一个事务，值加1；Leader选举后，低32位清零）。  
- **事务日志**：所有事务操作记录到日志文件，目录由dataLogDir配置，文件名以“第一条事务的zxid”为后缀，便于定位查找。ZooKeeper采用“磁盘空间预分配”策略减少磁盘Seek频率，提升事务处理性能；默认情况下，事务日志实时刷盘，也可配置为“写入内存文件流后定时批量刷盘”（但断电可能丢失数据）。  
- **事务快照**：记录ZooKeeper服务器某一时刻的全量内存数据，写入指定磁盘文件（目录由dataDir配置）。可通过snapCount配置“两次快照之间的事务操作个数”，当事务操作次数达到“snapCount/2 ~ snapCount”之间的随机值时，触发快照生成（随机值避免所有节点同时生成快照导致集群性能下降）。  

##### 2. 核心角色
- **Leader**：处理客户端写请求，同步事务到Follower；集群刚启动或Leader崩溃后，处于选举状态；  
- **Follower**：接受Leader的事务同步，存储到本地日志，为客户端提供读
请求；与Leader处于数据同步阶段；
- **Observer**：无选举和投票权，仅同步Leader数据，分担读请求压力，提升集群读性能；
- **节点状态**：
  - **LOOKING**：节点处于选主状态，不对外提供服务，直至选主结束；
  - **FOLLOWING**：节点为从节点，接受主节点更新并写入本地日志；
  - **LEADING**：节点为主节点，接受客户端更新，写入本地日志并复制到从节点。


### ZAB协议的常见误区
1. **“写入节点后的数据能立马被读到”是错误的**  
ZooKeeper的写请求必须通过Leader串行处理，且只要超过一半节点写入成功即可返回客户端“写入成功”；但所有节点均可提供读服务——若读请求被分配到“未同步最新数据的节点”（如写入成功的节点为1~3，读请求分配到4~5），则无法读取到最新数据。若需读取最新数据，可在读取前执行`sync`命令。  

2. **“ZooKeeper启动节点不能为偶数台”是错误的**  
ZooKeeper需“超过一半节点正常”才能工作：  
- 4台节点集群：半数以上正常节点数为3，最多允许1台节点故障；  
- 3台节点集群：半数以上正常节点数为2，同样最多允许1台节点故障。  
4台节点虽能工作，但比3台多一台机器成本，健壮性却相同，从成本角度不推荐使用偶数台节点。  


### ZAB协议的选举与同步过程
#### 一、发起投票的契机
1. 节点启动时；  
2. 节点运行期间无法与Leader保持连接时；  
3. Leader失去一半以上节点连接时。  


#### 二、事务一致性保证（类似两阶段提交）
以“客户端请求设置`/my/test`值为1”为例，流程如下：  
1. Leader生成对应的事务提议（proposal），分配全局唯一zxid（如当前zxid为`0x5000010`，提议zxid为`0x5000011`），将“set /my/test 1”（伪代码）写入本地事务日志；  
2. Leader将该事务日志同步到所有Follower；  
3. Follower收到事务提议后，将其写入本地事务日志，并向Leader反馈“同步完成”；  
4. Leader收到超过半数Follower的“同步完成”响应后，广播`commit`请求；  
5. Follower收到`commit`请求后，将zxid为`0x5000011`的事务应用到内存，完成数据一致性。  

**特殊场景处理**：  
- 场景1：Leader写入本地事务日志后，未发送同步请求即宕机——重启后作为Follower，因新Leader无此日志，该事务日志会被丢弃；  
- 场景2：Leader发送同步请求后，未广播`commit`即宕机——新Leader选举后，会继续完成该事务的`commit`，确保日志不丢失。  

### 如何保证事务

ZAB 协议类似于两阶段提交，客户端有一个写请求过来，例如设置 /my/test 值为 1，Leader 会生成
对应的事务提议（proposal）（当前 zxid为 0x5000010 提议的 zxid 为Ox5000011），现将 set
/my/test 1 （此处为伪代码）写入本地事务日志，然后 set /my/test 1 日志同步到所有的
follower。follower收到事务 proposal ，将 proposal 写入到事务日志。如果收到半数以上 follower 的
回应，那么广播发起 commit 请求。follower 收到 commit 请求后。会将文件中的 zxid ox5000011 应
用到内存中。

上面说的是正常的情况。有两种情况。第一种 Leader 写入本地事务日志后，没有发送同步请求，就
down 了。即使选主之后又作为 follower 启动。此时这种还是会日志会丢掉（原因是选出的 leader 无
此日志，无法进行同步）。第二种 Leader 发出同步请求，但是还没有 commit 就 down 了。此时这个
日志不会丢掉，会同步提交到其他节点中。
#### 三、服务器启动过程中的投票（以5台节点为例，编号1~5）
1. 节点1启动，发送投票请求无响应，处于LOOKING状态；  

2. 节点2启动，与节点1通信交换选举结果——因两者无历史数据（zxid相同），编号大的节点2胜出，但未达半数节点（仅2台），仍保持LOOKING状态；  

3. 节点3启动，与节点1、2通信——编号最大的节点3胜出，且参与选举的节点数（3台）达半数以上，节点3成为Leader；  


4. 节点4启动，与1~3通信，得知节点3为Leader且自身zxid更小，承认节点3的Leader角色，转为FOLLOWING状态；  

5. 节点5启动，流程同节点4，转为FOLLOWING状态。  


#### 四、服务器运行过程中的选主
1. 节点1发起投票，先投自己，进入LOOKING状态；  

2. 其他节点（如节点2）收到投票请求：  
   - 若自身处于LOOKING状态，广播自己的投票结果；  
   - 若自身不处于LOOKING状态（已知道Leader），告知节点1当前Leader，终止其选举；  
3. 节点1收到其他节点的投票结果：  
   - 若某节点zxid更大，清空原有投票，重新广播投票给该节点；  
   - 若某节点得票达半数以上，终止选举，该节点成为新Leader。  

**选举优先级**：优先比较zxid（zxid大的节点拥有最新数据）；若zxid相同（如系统刚启动），比较节点编号（编号大的胜出）。  


#### 五、选主后的同步过程
选主后，ZooKeeper进入状态同步阶段，将Leader的最新日志数据同步到其他节点（同步数据包含Follower已写入但未提交的日志）。同步前需初始化三个zxid值：  
- `peerLastZxid`：某Follower（Learner）最后处理的zxid；  
- `minCommittedLog`：Leader提议缓存队列（committedLog）中的最小zxid；  
- `maxCommittedLog`：Leader提议缓存队列（committedLog）中的最大zxid。  

根据`peerLastZxid`与`minCommittedLog`、`maxCommittedLog`的关系，采用三种同步策略：  

##### 1. 直接差异化同步
- **场景**：`peerLastZxid`介于`minCommittedLog`和`maxCommittedLog`之间（如Leader队列：`0x20001`、`0x20002`、`0x20003`、`0x20004`；Follower的`peerLastZxid`：`0x20002`）；  
- **操作**：Leader仅同步`0x20003`、`0x20004`两个提议给Follower，完成后发送`commit`指令。  


##### 2. 先回滚再差异化同步/仅回滚同步
- **场景**：Leader写入本地日志后未发送同步请求即宕机，重启后作为Follower参与同步；  
- **仅回滚同步**：若新Leader无该未同步日志（如旧Leader节点1的日志`0x20003`未发送，新Leader节点2的队列仅`0x20001`、`0x20002`），则节点1回滚到`0x20002`，废弃`0x20003`；  
- **先回滚再差异化同步**：若新Leader已处理新事务（如队列：`0x20001`、`0x20002`、`0x30001`、`0x30002`），则节点1先回滚到`0x20002`，再同步`0x30001`、`0x30002`。  


##### 3. 全量同步
- **场景**：`peerLastZxid`小于`minCommittedLog`，或Leader无提议缓存队列；  
- **操作**：Leader通过`SNAP`命令向Follower发送全量数据快照，Follower加载快照后完成同步。  


## 参考文献
1. https://www.cnblogs.com/zhang-qc/p/8688258.html  
2. https://blog.csdn.net/weixin_33725272/article/details/87947998  
3. http://ifeve.com/raft/  

